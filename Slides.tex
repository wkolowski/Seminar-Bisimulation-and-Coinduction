\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage{polski}
\usepackage[polish]{babel}
\usepackage{graphicx}
\usepackage{minted}

\title{Coinduction and topology:\\an (un)expected connection}
\author{Wojciech Kołowski}
\date{17 June 2020}

\usetheme{Darmstadt}

\begin{document}

\frame{\titlepage}
\frame{\tableofcontents}

\section{Review and critique}

\begin{frame}{Review of approaches to coinduction}
\begin{itemize}
	\item So far we have seen two approaches to coinduction and bisimulation:
	\item The LTS approach, in which a coinductive process is (re)presented using a particular kind of machine. This machine can be in any of a number of states and can transition between them by performing an appropriate action.
	\item The categorical approach, in which we are interested in coalgebras of endofunctors. In the final coalgebra $(\nu F, \alpha)$ the ``coinductively'' defined object is the carrier $\nu F$, and $\alpha$ takes the object apart, splitting into its constituent parts (it can also be seen as performing some observations on $\nu F$). Corecursion is a consequence of finality.
\end{itemize}
\end{frame}

\begin{frame}{The LTS approach: a critique 1/2}
\begin{itemize}
	\item I think that the LTS approach to coinduction and bisimulation is quite bad from the explanatory point of view, for a few reasons:
	\item First, it obscures the very important duality between induction and coinduction, which everybody wants to learn about instantly upon seeing the name ``coinduction''.
	\item Interlude: the right notion of equality for LTSes is of course graph isomorphism, as they are nothing more than labeled graphs -- static, immobile objects that prescribe actions and transitions, but don't act and don't transition.
\end{itemize}
\end{frame}

\begin{frame}{The LTS approach: a critique 2/2}
\begin{itemize}
	\item Second, the idea of bisimulation is a bit ad hoc and circular.
	\item Bisimulation was advertised in the book as the right notion of behavioural equality of LTSes, but it is in fact the right notion of equality for behaviours of LTSes. The behaviour of an LTS is its dynamic aspect -- where the actions and transitions take place.
	\item However, to formally define such a notion of behaviour, we need coinduction in the first place (or else we will miss ``infinite'' phenomena). Thus, there is some kind of circularity in explaining coinduction using LTSes, even if only conceptual.
\end{itemize}
\end{frame}

\begin{frame}{The categorical approach: a critique}
\begin{itemize}
	\item The categorical approach is much better, as it makes the duality between induction and coinduction more explicit and also doesn't give the false impression that coinduction is about automata.
	\item However, it is not without faults:
	\item By using the machinery of category theory it makes coinduction seem more magical and arcane than it really is. It is unlikely to be enlightening to ordinary programmers and people with category theory disability.
	\item It makes the operational and computational aspects of corecursion less explicit.
	\item It does not provide a nice syntax/notation for corecursive definitions (even though it does provide $\nu X. F(X)$ for objects) -- and that's very important! ``Notation is the tool of thought'', they say.
\end{itemize}
\end{frame}

\section{An intuitive approach to coinduction}

\begin{frame}{How to explain coinduction to 5 year olds}
\begin{itemize}
	\item I think that the most natural way of explaining coinduction is to refer to an informal version of the duality with induction, explain it in depth and then present lots of examples and exercises to build the right intuitions.
	\item So, let's do just that! 
\end{itemize}
\end{frame}

\begin{frame}{The duality}
\begin{tabular}{ | p{3cm} | p{3cm} | p{3cm} | }
	\hline
	feature $\downarrow$ & induction & coinduction \\\hline
	shape & sum (of products) & product (of sums) \\\hline
	basic activity & construction & deconstruction (observation) \\\hline
	derived activity & deconstruction (observation) & construction \\\hline
	easy to define functions with & inductive domain & coinductive codomain \\\hline
	such that every (co)recursive call & shrinks the principal argument & grows the result \\\hline
	thus these functions are & terminating & productive \\\hline
	evaluation & possibly eager & necessarily lazy \\\hline
	tree height & necessarily finite & possibly infinite \\\hline
	% tree width & any & any \\\hline
\end{tabular}
\end{frame}

\begin{frame}{Explaining the first half of the duality}
\begin{itemize}
	\item The first half of the table can be restated in terms of category theory and logic/type theory.
	\item In categorical terms it means that inductives have a ``mapping-out'' universal property, i.e. they are colimits, whereas coinductives have a ``mapping-in'' universal property, i.e. they are limits.
	\item In logical terms, we can say that inductives have positive polarity and coinductives have negative polarity. For nice explanations of polarity, see \url{https://existentialtype.wordpress.com/2012/08/25/polarity-in-type-theory/} and \url{http://noamz.org/talks/logpolpro.pdf}.
	\item All of this can also be restated in less scary terms.
\end{itemize}
\end{frame}

\begin{frame}{(Co)induction, (co)limits, positive and negative types 1/2}
\begin{itemize}
	\item Inductives are determined by ways of constructing their elements and the elimination principle is a derived notion whose purpose is to say ``the only things you can build come from the introduction principles (constructors)''.
	\item Coinductives are determined by ways of observing their elements and the introduction principle is a derived notion whose purpose is to say ``the only things you can observe come from the elimination principles''.
	\item For programmers this basically means that inductives are data (similar to the stuff stored in databases), whereas coinductives are interactive processes (like operating systems or web servers).
\end{itemize}
\end{frame}

\begin{frame}{(Co)induction, (co)limits, positive and negative types 2/2}
\begin{itemize}
	\item The type \texttt{bool} is an inductive type with two constructors \texttt{true} and \texttt{false}. Knowing this we can derive an elimination principle, which amounts to an \texttt{if-then-else} expression (but dependently typed!).
	\item Imagine a type of web servers that can only handle requests for pictures of funny cats (this is the elimination principle). From this description we know that there must be something in the web server that is responsible for handling these requests and thus the derived introduction principle specifies all the possible ways of constructing that thing.
\end{itemize}
\end{frame}

\begin{frame}{Explaining the second half of the duality}
\begin{itemize}
	\item The second half of the duality is quite clear and doesn't need to be demystified as much as the first, but there are two philosophical misconceptions to be addressed.
	\item The first is about lazy and strict languages.
	\item The second is about ``infinite loops''.
\end{itemize}
\end{frame}

\begin{frame}{Laziness and strictness 1/3}
\begin{itemize}
	\item Inductives can be evaluated both lazily and eagerly, buth since they are data which is most often meant to be passed to some function for further processing, it makes more sense for them to be eager.
	\item Because coinductives are possibly infinite, they can't be evaluated eagerly and thus any language that incorporates them will have some form of lazy evaluation.
	\item We may think that inductive types are (or at least should) be ``eager'', whereas coinductive types are ``lazy''.
\end{itemize}
\end{frame}

\begin{frame}{Laziness and strictness 2/3}
\begin{itemize}
	\item An interesting case is the product type, which can be defined both inductively and coinductively.
	\item Elements of inductive products are constructed by pairing two things. The derived eliminator says that we can pattern match on the pair to get them back and then use them to construct something else.
	\item Elements of coinductive products are eliminated using projections. The derived introduction rule says that we must provide everything that can be projected out, so it is pairing too.
	\item Both product types are isomorphic, but inductive products are best thought of as ``eager products'', whereas coinductive products are best thought of as ``lazy products''.
\end{itemize}
\end{frame}

\begin{frame}{Lazines and strictness 3/3}
\begin{itemize}
	\item \textbf{Laziness and strictness are properties of types, not of languages}
	\item Haskell is usually said to be a ``lazy language'', but in reality it's just that its types are lazy by default. Given some strictness annotations we can define strict types or even mixed strict-lazy types.
	\item OCaml or StandardML are usually said to be ``strict'' languages, but it's just that their types are strict by default. We can make a type \texttt{'a} lazy by turning it into a function type with unit domain: \texttt{unit -> 'a}.
\end{itemize}
\end{frame}

\begin{frame}{Termination and productivity 1/3}
\begin{itemize}
	\item In programmers' collective consciousness there is the term ``infinite loop'', usually applied in two distinct situations.
	\item Sometimes it looks like \texttt{while(true) $\lbrace$...$\rbrace$}. In such cases the infinite loop was programmed intentionally. Its purpose may be to, for example, implement a server that is waiting for requests.
	\item Other times it looks like an ordinary loop, but the stopping condition will never be met. In such cases the infinite loop was programmed by mistake and thus is a bug.
	\item This term would also be applied to describe an erroneous implementation of recursive factorial on integers with the base case missing, even though there isn't any kind of loop going on.
	\item Note that this term is very one-sided -- terminating programs aren't called ``finite loops''.
\end{itemize}
\end{frame}

\begin{frame}{Termination and productivity 2/3}
\begin{itemize}
	\item \textbf{The term ``infinite loop'' is considered harmful, because it conflates two separate notions: termination and productivity.}
	\item Termination is a property that pertains only to recursive functions.
	\item Each recursive call must shrink the input and may produce a part of the output.
	\item Therefore all recursive functions terminate.
	\item Productivity is a property that pertains only to corecursive functions.
	\item Each corecursive call may do anything with the input and must produce a part of the output.
	\item Therefore all corecursive functions are productive.
\end{itemize}
\end{frame}

\begin{frame}{Termination and productivity 3/3}
\begin{itemize}
	\item To sum it up:
	\item Bugged implementation of factorial on integers without the base case: recursive, nonterminating, not ok.
	\item Correct implementation of factorial on natural numbers: recursive, terminating, ok.
	\item Correct implementation of a web server that serves pictures of funny cats: corecursive, productive, ok.
	\item A web server that hangs for some requests: corecursive, nonproductive, not ok.
\end{itemize}
\end{frame}

\begin{frame}{A comparatory example}
\begin{itemize}
	\item How does the duality play out in practice? Let's see an example in Coq!
	\item The code for this example, named \texttt{Snippet1.v}, is available from the GitHub repo of this talk: \url{https://github.com/wkolowski/Seminar-Bisimulation-and-Coinduction}
\end{itemize}
\end{frame}

\section{Coinduction in type theory}

\begin{frame}{Rescuing the duality}
\begin{itemize}
	\item Induction principle: \texttt{forall i : I, P i}
	%8. Koindukcja strukturalna: nie ma w Coqu, ale jej odpowiednikiem jest najprawdopodobniej bipodobieństwo = równość (patrz prace chyba Setzera).
\end{itemize}
\end{frame}

\section{Finite, infinite and searchable types}

\begin{frame}{}
\begin{itemize}
	\item $\texttt{Searchable}(A) :\equiv \Pi p : A \to \mathbf{2}, \left(\Sigma x : A, p(x) = true\right) + \left(\Pi x : A, p(x) = false\right)$
\end{itemize}
\end{frame}

\end{document}
